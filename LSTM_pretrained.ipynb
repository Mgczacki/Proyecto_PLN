{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import ast\n",
    "from termcolor import colored\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from tqdm import trange\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# For my jupyter theme\n",
    "%config InlineBackend.print_figure_kwargs = {'facecolor':'white'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# torch.cuda.current_device(), torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = 'cuda:0' \n",
    "else:  \n",
    "    dev = 'cpu'\n",
    "#dev = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen:\n",
    "\n",
    "1. Quitando símbolos tóxicos (\", espacios, !, etc) tenemos los siguientes resultados:\n",
    "  - LSTM: *train*=0.6386, *test*=0.5886 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "2. Quitando símbolos tóxicos (\", espacios, !, etc) y agregando como completamente tóxicos todos los que no tienen índices (los que viene como [ ]):\n",
    "  - LSTM: *train*=0.6386, *test*=0.5886 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "3. Sin quitar símbolos tóxicos (\", espacios, !, etc), es decir, dataset original:\n",
    "  - LSTM: *train*=0.570, *test*=0.531 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "4. Sin quitar símbolos tóxicos (\", espacios, !, etc) y agregando como completamente tóxicos todos los que no tienen índices (los que viene como [ ]):\n",
    "  - LSTM: *train*=0.570, *test*=0.531 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "Para la LSTM lo mejor parece ser la opción 1. Lo mejor hasta ahora es con `(EMBEDDING_DIM = 20 HIDDEN_DIM = 20 EPOCHS = 25)` (test=0.581, aunque no se fijó la semilla y no se puede reproducir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (color_toxic_words, remove_symbols, completely_toxic, separate_words, \n",
    "                   get_index_toxic_words, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Datos/tsd_train.csv', converters={'spans':ast.literal_eval})\n",
    "test = pd.read_csv('Datos/tsd_trial.csv', converters={'spans':ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spans_clean'] = train['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pondremos los que tienen [ ] como completamente tóxicos para ver qué sale.\n",
    "\n",
    "clean_spans = [completely_toxic(span, text) for (span, text) in zip(train['spans'], train['text'])]\n",
    "train['spans_clean'] = clean_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos símbolos\n",
    "\n",
    "indices_clean = [remove_symbols(index, text) for index,text in zip(train['spans'], train['text'])]\n",
    "train['spans_clean'] = indices_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos a minúscula\n",
    "train['text'] = train['text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "\n",
    "text_field = Field(\n",
    "    tokenize='basic_english', \n",
    "    lower=True\n",
    ")\n",
    "label_field = Field(sequential=False, use_vocab=False)\n",
    "# sadly have to apply preprocess manually\n",
    "preprocessed_text = train['text'].apply(lambda x: text_field.preprocess(x))\n",
    "# load fastext simple embedding with 300d\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='glove.twitter.27B.25d'\n",
    ")\n",
    "# get the vocab instance\n",
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126, 1322, 6, 3243, 2117, 412, 7, 796, 6, 1014, 85, 1085, 0, 3944]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices(['another', 'violent', 'and', 'aggressive', 'immigrant', 'killing', 'a', 'innocent', 'and', 'intelligent', 'us', 'citizen', '....', 'sarcasm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['spans_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SpansDataset(Dataset):\n",
    "    \"\"\"Spans dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        score = np.array(self.df.iloc[idx, 0]).reshape(1, -1)\n",
    "        features = np.array(self.df.iloc[idx, 1:]).reshape(1, -1)\n",
    "        \n",
    "        sample = {'features': features, 'score': score}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index, text in zip(train['spans_clean'], train['text']):\n",
    "    toxic_words = [text[i[0]:i[-1]+1] for i in separate_words(index) if len(index) > 0]\n",
    "#     print(toxic_words)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in toxic_words:\n",
    "            tagged_tokens.append('toxic')\n",
    "            # Removemos en caso de que se repita posteriormente pero esté como 'non_toxic'\n",
    "            toxic_words.remove(token) \n",
    "        else:\n",
    "            tagged_tokens.append('non_toxic')\n",
    "            \n",
    "    train_data.append((tokens, tagged_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for index, text in zip(test['spans'], test['text']):\n",
    "    toxic_words = [text[i[0]:i[-1]+1] for i in separate_words(index) if len(index) > 0]\n",
    "#     print(toxic_words)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in toxic_words:\n",
    "            tagged_tokens.append('toxic')\n",
    "            # Removemos en caso de que se repita posteriormente pero esté como 'non_toxic'\n",
    "            toxic_words.remove(token) \n",
    "        else:\n",
    "            tagged_tokens.append('non_toxic')\n",
    "            \n",
    "    test_data.append((tokens, tagged_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    idxs = vocab.lookup_indices(seq)      # Si no está que lo ponga como 'UNK'\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=dev)\n",
    "def prepare_sequence_tags(seq):\n",
    "    tag_to_ix = {\"non_toxic\": 0, \"toxic\": 1} \n",
    "    idxs = [tag_to_ix[s] for s in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word_to_ix = {'UNK': 0}\n",
    "\n",
    "for sent, tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:              # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\"\"\"            \n",
    "tag_to_ix = {\"non_toxic\": 0, \"toxic\": 1}        # Assign each tag with a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, weight, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Usar otro embedding, tratar con alguno que haya en 'transformers'\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weight)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_LSTM(text):\n",
    "    ix_to_tag = {0: 'non_toxic', 1: 'toxic'}\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = prepare_sequence(words)\n",
    "        tag_scores = model(inputs)\n",
    "        tags = [np.argmax(x.cpu().numpy()) for x in tag_scores]\n",
    "\n",
    "        tagged_sentence = [(word, ix_to_tag[tag]) for word,tag in zip(words, tags)]\n",
    "\n",
    "    return tagged_sentence\n",
    "\n",
    "def f1_scores(df):\n",
    "    scores_LSTM = []\n",
    "    for gold_index, text in df.values:\n",
    "        tagged_sentence = tagger_LSTM(text)   \n",
    "        prediction_index = get_index_toxic_words(text.lower(), tagged_sentence)\n",
    "        scores_LSTM.append(f1(prediction_index, gold_index))\n",
    "        \n",
    "    return np.mean(scores_LSTM)\n",
    "\n",
    "def plot_loss_and_score(train_loss, test_loss, f1_scores_train, f1_scores_test, show=True):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(18,7))\n",
    "\n",
    "    ax0.plot(np.arange(1, len(train_loss) + 1), train_loss, marker='o', label='train_loss')\n",
    "    ax0.plot(np.arange(1, len(test_loss) + 1), test_loss, marker='o', label='validation loss')\n",
    "    ax0.set_xlabel('Epochs')\n",
    "    ax0.set_ylabel('Loss')\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1.plot(np.arange(1, len(f1_scores_train) + 1), f1_scores_train, \n",
    "             marker='o', label='f1 score in train')\n",
    "    ax1.plot(np.arange(1, len(f1_scores_test) + 1), f1_scores_test, \n",
    "             marker='o', label='f1 score in test')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('F1 score')\n",
    "    ax1.legend()\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(20566, 25)\n",
       "  (lstm): LSTM(25, 30)\n",
       "  (hidden2tag): Linear(in_features=30, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_DIM = 30\n",
    "embedding_dim = 25\n",
    "model = LSTMTagger(embedding_dim, vocab.vectors, HIDDEN_DIM, len(vocab.vectors), len(tag_to_ix))\n",
    "model.to(torch.device(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "#optimizer = optim.Adam(model.parameters(), weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = [0]\n",
    "training_loss = [0]\n",
    "f1_scores_train = [0]\n",
    "f1_scores_dev = [0]\n",
    "stop_after_best = 100\n",
    "best_l = None\n",
    "best_tl = None\n",
    "worst_l = 0\n",
    "worst_tl = 0\n",
    "worst_l_f1 = 0\n",
    "best_l_f1 = None\n",
    "worst_tl_f1 = 0\n",
    "last_epoch_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "Current epoch: 3\n",
      "Last model save was in epoch 1\n",
      "Stopping training in: 99 epochs.\n",
      "###############################################\n",
      "[Best iter] test F1 is: 0.06109081748331024\n",
      "[Best iter] dev F1 is: 0.06231884057971015\n",
      "###############################################\n",
      "[Last iter] training F1 was: 0.06109081748331024\n",
      "[Last iter] dev. F1 was: 0.06231884057971015\n",
      "###############################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 done.\n",
      "Sentence 1000 done.\n",
      "Sentence 2000 done.\n",
      "Sentence 3000 done.\n",
      "Sentence 4000 done.\n",
      "Sentence 5000 done.\n",
      "Sentence 6000 done.\n",
      "Sentence 7000 done.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "stop_after_best = 100\n",
    "#Nombre del archivo de backup\n",
    "savefile = 'best-model.pt'\n",
    "\n",
    "epochs_without_change = 0\n",
    "epochs = len(loss_per_epoch)\n",
    "\n",
    "while epochs_without_change < stop_after_best:  # itero hasta que no mejore el desempeño en dev por 100 épocas\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"###############################################\")\n",
    "    print(\"Current epoch: \" + str(epochs))\n",
    "    print(\"Last model save was in epoch \" + str(last_epoch_save))\n",
    "    print(\"Stopping training in: \" + str(stop_after_best - epochs_without_change) + \" epochs.\")\n",
    "    print(\"###############################################\")\n",
    "    print(\"[Best iter] test F1 is: \" + str(best_tl))\n",
    "    print(\"[Best iter] dev F1 is: \" + str(best_l))\n",
    "    print(\"###############################################\")\n",
    "    print(\"[Last iter] training F1 was: \" + str(f1_scores_train[-1]))\n",
    "    print(\"[Last iter] dev. F1 was: \" + str(f1_scores_dev[-1]))\n",
    "    print(\"###############################################\")\n",
    "    #Dibujo lo que puedo\n",
    "    plt.plot(range(epochs), loss_per_epoch)\n",
    "    plt.plot(range(epochs), training_loss)\n",
    "    plt.plot(range(epochs), f1_scores_dev)\n",
    "    plt.plot(range(epochs), f1_scores_train)\n",
    "    plt.legend([\"Dev Loss\", \"Training Loss\", \"Dev F1\", \"Training F1\"])\n",
    "    plt.show()\n",
    "    tl = 0\n",
    "    for i, v in enumerate(train_data):\n",
    "        sentence, tags = v\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence)\n",
    "        targets = prepare_sequence_tags(tags) \n",
    "\n",
    "        #print(targets)\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "\n",
    "        loss = criterion(tag_scores, targets)\n",
    "        tl += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Sentence {} done.\".format(i))\n",
    "    tl /= len(train_data)\n",
    "    l = 0\n",
    "    #evaluar el modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in test_data:\n",
    "            sentence_in = prepare_sequence(sentence)\n",
    "            targets = prepare_sequence_tags(tags) \n",
    "\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            loss_test = criterion(tag_scores, targets)\n",
    "            l += loss_test.item()\n",
    "    model.train()\n",
    "    l /= len(test_data)\n",
    "    \n",
    "    f1_t = f1_scores(train[['spans', 'text']])\n",
    "    f1_d = f1_scores(test[['spans', 'text']])\n",
    "    \n",
    "    epochs_without_change += 1\n",
    "    if best_l is None or best_l < f1_d:\n",
    "        torch.save(model, savefile)\n",
    "        best_l = f1_d\n",
    "        best_tl = f1_t\n",
    "        epochs_without_change = 0\n",
    "        last_epoch_save = epochs\n",
    "        print(\"Model improved, saved.\")\n",
    "    #Para graficar con una escala coherente.\n",
    "    if(f1_d > worst_l_f1):\n",
    "        worst_l_f1 = f1_d\n",
    "        f1_scores_dev[0] = worst_l_f1\n",
    "    if(f1_t > worst_tl_f1):\n",
    "        worst_tl_f1 = f1_t\n",
    "        f1_scores_train[0] = worst_tl_f1\n",
    "    if(tl > worst_tl):\n",
    "        worst_tl = tl\n",
    "        training_loss[0] = worst_tl\n",
    "    if(l > worst_l):\n",
    "        worst_l = l\n",
    "        loss_per_epoch[0] = worst_l\n",
    "    #Rastreo las perdidas\n",
    "    loss_per_epoch.append(l)\n",
    "    training_loss.append(tl)\n",
    "    f1_scores_train.append(f1_t)\n",
    "    f1_scores_dev.append(f1_d)\n",
    "    #Rastreo la época actual\n",
    "    epochs += 1\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-49a52e3b7330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_scores_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_scores_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-f1: {:.4f} \\n test-f1: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_scores_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_scores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Images/preprocesamiento-4.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss_and_score(train_loss, test_loss, f1_scores_train, f1_scores_test, show=False)\n",
    "plt.title('train-f1: {:.4f} \\n test-f1: {:.4f}'.format(np.max(f1_scores_train), np.max(f1_scores_test)))\n",
    "plt.savefig('Images/preprocesamiento-4.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Watch people die from taking away their healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tens years ago i contacted the PDR and suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The parallels between the ANC and the Sicilian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intel Community: ‘How can we work for a Presid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  That's right. They are not normal. And I am st...\n",
       "1  \"Watch people die from taking away their healt...\n",
       "2  tens years ago i contacted the PDR and suggest...\n",
       "3  The parallels between the ANC and the Sicilian...\n",
       "4  Intel Community: ‘How can we work for a Presid..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = pd.read_csv('Datos/tsd_test.csv')\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_evaluation = []\n",
    "for text in evaluation['text']:\n",
    "    tagged_sentence = tagger_LSTM(text)   \n",
    "    prediction_index = get_index_toxic_words(text.lower(), tagged_sentence)\n",
    "    indices_evaluation.append(prediction_index)\n",
    "#     print(colored('Pred: ', color='cyan', attrs=['bold']) + \n",
    "#           color_toxic_words(prediction_index, text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[133, 134, 135, 136, 137, 138, 140, 141, 142, ...</td>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[54, 55, 56, 57, 59, 60, 61, 62, 81, 82, 83, 8...</td>\n",
       "      <td>\"Watch people die from taking away their healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[483, 484, 485, 486, 487, 488, 489, 490, 492, ...</td>\n",
       "      <td>tens years ago i contacted the PDR and suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[413, 414, 415, 416, 417, 418, 419, 420]</td>\n",
       "      <td>The parallels between the ANC and the Sicilian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[126, 127, 128, 129, 130, 271, 272, 273, 274, ...</td>\n",
       "      <td>Intel Community: ‘How can we work for a Presid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               spans  \\\n",
       "0  [133, 134, 135, 136, 137, 138, 140, 141, 142, ...   \n",
       "1  [54, 55, 56, 57, 59, 60, 61, 62, 81, 82, 83, 8...   \n",
       "2  [483, 484, 485, 486, 487, 488, 489, 490, 492, ...   \n",
       "3           [413, 414, 415, 416, 417, 418, 419, 420]   \n",
       "4  [126, 127, 128, 129, 130, 271, 272, 273, 274, ...   \n",
       "\n",
       "                                                text  \n",
       "0  That's right. They are not normal. And I am st...  \n",
       "1  \"Watch people die from taking away their healt...  \n",
       "2  tens years ago i contacted the PDR and suggest...  \n",
       "3  The parallels between the ANC and the Sicilian...  \n",
       "4  Intel Community: ‘How can we work for a Presid...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['spans'] = indices_evaluation\n",
    "evaluation = evaluation[['spans', 'text']]\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la evaluación se debe subir un zip con un archivo txt de la siguiente manera (al final subir el archivo `spans-pred.zip` que se produce):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: spans-pred.txt (deflated 82%)\r\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluation['spans'].tolist()\n",
    "ids = evaluation.index.tolist()\n",
    "\n",
    "with open(\"spans-pred.txt\", \"w\") as out:\n",
    "    for uid, text_scores in zip(ids, predictions):\n",
    "        out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")\n",
    "        \n",
    "# Zip the predictions\n",
    "! zip -r spans-pred.zip ./spans-pred.* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
