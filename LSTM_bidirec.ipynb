{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import ast\n",
    "from termcolor import colored\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from tqdm import trange\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# For my jupyter theme\n",
    "%config InlineBackend.print_figure_kwargs = {'facecolor':'white'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# torch.cuda.current_device(), torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = 'cuda:0' \n",
    "else:  \n",
    "    dev = 'cpu'\n",
    "#dev = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen:\n",
    "\n",
    "1. Quitando símbolos tóxicos (\", espacios, !, etc) tenemos los siguientes resultados:\n",
    "  - LSTM: *train*=0.6386, *test*=0.5886 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "2. Quitando símbolos tóxicos (\", espacios, !, etc) y agregando como completamente tóxicos todos los que no tienen índices (los que viene como [ ]):\n",
    "  - LSTM: *train*=0.6386, *test*=0.5886 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "3. Sin quitar símbolos tóxicos (\", espacios, !, etc), es decir, dataset original:\n",
    "  - LSTM: *train*=0.570, *test*=0.531 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "4. Sin quitar símbolos tóxicos (\", espacios, !, etc) y agregando como completamente tóxicos todos los que no tienen índices (los que viene como [ ]):\n",
    "  - LSTM: *train*=0.570, *test*=0.531 `(EMBEDDING_DIM = 20, HIDDEN_DIM = 20, EPOCHS = 25)`\n",
    "  \n",
    "Para la LSTM lo mejor parece ser la opción 1. Lo mejor hasta ahora es con `(EMBEDDING_DIM = 20 HIDDEN_DIM = 20 EPOCHS = 25)` (test=0.581, aunque no se fijó la semilla y no se puede reproducir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (color_toxic_words, remove_symbols, completely_toxic, separate_words, \n",
    "                   get_index_toxic_words, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Datos/tsd_train.csv', converters={'spans':ast.literal_eval})\n",
    "test = pd.read_csv('Datos/tsd_trial.csv', converters={'spans':ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spans_clean'] = train['spans']\n",
    "test['spans_clean'] = test['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pondremos los que tienen [ ] como completamente tóxicos para ver qué sale.\n",
    "\n",
    "clean_spans = [completely_toxic(span, text) for (span, text) in zip(train['spans'], train['text'])]\n",
    "train['spans_clean'] = clean_spans\n",
    "\n",
    "clean_spans = [completely_toxic(span, text) for (span, text) in zip(test['spans'], test['text'])]\n",
    "test['spans_clean'] = clean_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos símbolos\n",
    "\n",
    "indices_clean = [remove_symbols(index, text) for index,text in zip(train['spans'], train['text'])]\n",
    "train['spans_clean'] = indices_clean\n",
    "\n",
    "indices_clean = [remove_symbols(index, text) for index,text in zip(test['spans'], test['text'])]\n",
    "test['spans_clean'] = indices_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos a minúscula\n",
    "train['text'] = train['text'].apply(lambda x:x.lower())\n",
    "\n",
    "# Pasamos a minúscula\n",
    "test['text'] = test['text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/anaconda3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "\n",
    "text_field = Field(\n",
    "    tokenize='basic_english', \n",
    "    lower=True\n",
    ")\n",
    "label_field = Field(sequential=False, use_vocab=False)\n",
    "# sadly have to apply preprocess manually\n",
    "preprocessed_text = train['text'].apply(lambda x: text_field.preprocess(x))\n",
    "# load fastext simple embedding with 300d\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='glove.twitter.27B.200d'\n",
    ")\n",
    "# get the vocab instance\n",
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126, 1322, 6, 3243, 2117, 412, 7, 796, 6, 1014, 85, 1085, 0, 3944]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices(['another', 'violent', 'and', 'aggressive', 'immigrant', 'killing', 'a', 'innocent', 'and', 'intelligent', 'us', 'citizen', '....', 'sarcasm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['spans_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spans          [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n",
       "text           another violent and aggressive immigrant killi...\n",
       "spans_clean    [8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 20, 21,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index, text in zip(train['spans_clean'], train['text']):\n",
    "    toxic_words = [text[i[0]:i[-1]+1] for i in separate_words(index) if len(index) > 0]\n",
    "#     print(toxic_words)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in toxic_words:\n",
    "            tagged_tokens.append('toxic')\n",
    "            # Removemos en caso de que se repita posteriormente pero esté como 'non_toxic'\n",
    "            toxic_words.remove(token) \n",
    "        else:\n",
    "            tagged_tokens.append('non_toxic')\n",
    "            \n",
    "    train_data.append((tokens, tagged_tokens, text, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for index, text in zip(test['spans'], test['text']):\n",
    "    toxic_words = [text[i[0]:i[-1]+1] for i in separate_words(index) if len(index) > 0]\n",
    "#     print(toxic_words)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in toxic_words:\n",
    "            tagged_tokens.append('toxic')\n",
    "            # Removemos en caso de que se repita posteriormente pero esté como 'non_toxic'\n",
    "            toxic_words.remove(token) \n",
    "        else:\n",
    "            tagged_tokens.append('non_toxic')\n",
    "            \n",
    "    test_data.append((tokens, tagged_tokens, text, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    idxs = vocab.lookup_indices(seq)      # Si no está que lo ponga como 'UNK'\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=dev)\n",
    "def prepare_sequence_tags(seq):\n",
    "    tag_to_ix = {\"non_toxic\": 0, \"toxic\": 1} \n",
    "    idxs = [tag_to_ix[s] for s in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SpansDataset(Dataset):\n",
    "    \"\"\"Spans dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        text = prepare_sequence(self.data[idx][0])\n",
    "        spans = prepare_sequence_tags(self.data[idx][1])\n",
    "        sample = {'tokenized' : self.data[idx][0], 'original_text' : self.data[idx][2], 'text': text, 'spans': spans, 'true_index' : self.data[idx][3]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "train_ds = SpansDataset(train_data)\n",
    "test_ds = SpansDataset(test_data)\n",
    "\n",
    "trainloader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "#Test is dev in reality\n",
    "testloader = DataLoader(test_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word_to_ix = {'UNK': 0}\n",
    "\n",
    "for sent, tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:              # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\"\"\"            \n",
    "tag_to_ix = {\"non_toxic\": 0, \"toxic\": 1}        # Assign each tag with a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "\"\"\"\n",
    "try2\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, weight, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Usar otro embedding, tratar con alguno que haya en 'transformers'\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weight)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = 2, dropout=0.05, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\"\"\"\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, stacked_layers, dropout_p, weight, hidden_dim, vocab_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stacked_layers = stacked_layers\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weight)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = stacked_layers,\n",
    "                            dropout = dropout_p,\n",
    "                            bidirectional = True)\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        output, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        x = F.relu(self.fc1(output.view(len(sentence), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_toxic_words(sentence, tagged_sentence):\n",
    "    toxic_indices = []   \n",
    "    m = 0\n",
    "    #tag_to_ix = {\"non_toxic\": 0, \"toxic\": 1}\n",
    "    for word_tag in tagged_sentence:\n",
    "        word, tag = word_tag    \n",
    "        if tag == 1:#toxic\n",
    "            # Si la palabra tóxica aparece 2 o más veces ésto solo dará la primera \n",
    "            # aparición, hay que arreglar eso pero por lo mientras sirve\n",
    "            # word_indices = [sentence.find(word) + i for i in range(len(word))]\n",
    "            # toxic_indices.append(word_indices)\n",
    "            \n",
    "            # Así parece evitarf1_torch el problema de la palabra repetida\n",
    "            word_indices = [m + sentence[m:].find(word) + i for i in range(len(word))]\n",
    "            toxic_indices.append(word_indices)\n",
    "            m += sentence[m:].find(word) + len(word) + 1\n",
    "            \n",
    "    toxic_indices = [val for sublist in toxic_indices for val in sublist]\n",
    "    #Unir espacios y otras cosas para que suba el F1    \n",
    "    return toxic_indices\n",
    "\n",
    "\n",
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
    "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
    "    :param predictions: a list of predicted offsets\n",
    "    :param gold: a list of offsets serving as the ground truth\n",
    "    :return: a score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1. if len(predictions) == 0 else 0.\n",
    "    if len(predictions) == 0:\n",
    "        return 0.\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)\n",
    "\n",
    "def f1_scores(pred, true_index, tokenized, text):\n",
    "    scores_LSTM = 0\n",
    "    for i in range(len(pred)):\n",
    "        tags = [1 if x > 0.5 else 0 for x in pred[i]]\n",
    "        tagged_sentence = list(zip(tokenized[i], tags))\n",
    "        prediction_index = get_index_toxic_words(text[i], tagged_sentence)\n",
    "        scores_LSTM += f1(prediction_index, true_index[i])\n",
    "    return scores_LSTM/len(pred)\n",
    "\n",
    "def plot_loss_and_score(train_loss, test_loss, f1_scores_train, f1_scores_test, show=True):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(18,7))\n",
    "\n",
    "    ax0.plot(np.arange(1, len(train_loss) + 1), train_loss, marker='o', label='train_loss')\n",
    "    ax0.plot(np.arange(1, len(test_loss) + 1), test_loss, marker='o', label='validation loss')\n",
    "    ax0.set_xlabel('Epochs')\n",
    "    ax0.set_ylabel('Loss')\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1.plot(np.arange(1, len(f1_scores_train) + 1), f1_scores_train, \n",
    "             marker='o', label='f1 score in train')\n",
    "    ax1.plot(np.arange(1, len(f1_scores_test) + 1), f1_scores_test, \n",
    "             marker='o', label='f1 score in test')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('F1 score')\n",
    "    ax1.legend()\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(20566, 200)\n",
       "  (lstm): LSTM(200, 600, num_layers=6, dropout=0.15, bidirectional=True)\n",
       "  (fc1): Linear(in_features=1200, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#0.61 en dev loss\n",
    "HIDDEN_DIM = 250\n",
    "embedding_dim = len(vocab.vectors[0])\n",
    "model = LSTMTagger(embedding_dim, vocab.vectors, HIDDEN_DIM, len(vocab.vectors), len(tag_to_ix))\n",
    "model.to(torch.device(dev))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#try3 #0.65 en dev loss, acercandose\n",
    "HIDDEN_DIM = 600\n",
    "embedding_dim = len(vocab.vectors[0])\n",
    "model = LSTMTagger(embedding_dim, 6, 0.15, vocab.vectors, HIDDEN_DIM, len(vocab.vectors), len(tag_to_ix))\n",
    "model.to(torch.device(dev))\n",
    "\"\"\"\n",
    "#try4\n",
    "HIDDEN_DIM = 600\n",
    "embedding_dim = len(vocab.vectors[0])\n",
    "model = LSTMTagger(embedding_dim, 6, 0.15, vocab.vectors, HIDDEN_DIM, len(vocab.vectors))\n",
    "model.to(torch.device(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('best-model-try2.pt').to(torch.device(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), momentum = 0.85, lr=0.002, weight_decay=1e-9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002, weight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = [0]\n",
    "training_loss = [0]\n",
    "f1_scores_train = [0]\n",
    "f1_scores_dev = [0]\n",
    "stop_after_best = 100\n",
    "best_l = None\n",
    "best_tl = None\n",
    "worst_l = 0\n",
    "worst_tl = 0\n",
    "worst_l_f1 = 0\n",
    "best_l_f1 = None\n",
    "worst_tl_f1 = 0\n",
    "last_epoch_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: GeForce GTX 1080\n",
      "###############################################\n",
      "Current epoch: 1\n",
      "Last model save was in epoch 0\n",
      "Stopping training in: 100 epochs.\n",
      "###############################################\n",
      "[Best iter] test F1 is: None\n",
      "[Best iter] dev F1 is: None\n",
      "###############################################\n",
      "[Last iter] training F1 was: 0\n",
      "[Last iter] dev. F1 was: 0\n",
      "###############################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 done.\n",
      "Sentence 1000 done.\n",
      "Sentence 2000 done.\n",
      "Sentence 3000 done.\n",
      "Sentence 4000 done.\n",
      "Sentence 5000 done.\n",
      "Sentence 6000 done.\n",
      "Sentence 7000 done.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "stop_after_best = 100\n",
    "#Nombre del archivo de backup\n",
    "savefile = 'best-model-try4.pt'\n",
    "\n",
    "epochs_without_change = 0\n",
    "epochs = len(loss_per_epoch)\n",
    "\n",
    "while epochs_without_change < stop_after_best:  # itero hasta que no mejore el desempeño en dev por 100 épocas\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"Training on: \" + torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    print(\"###############################################\")\n",
    "    print(\"Current epoch: \" + str(epochs))\n",
    "    print(\"Last model save was in epoch \" + str(last_epoch_save))\n",
    "    print(\"Stopping training in: \" + str(stop_after_best - epochs_without_change) + \" epochs.\")\n",
    "    print(\"###############################################\")\n",
    "    print(\"[Best iter] test F1 is: \" + str(best_tl))\n",
    "    print(\"[Best iter] dev F1 is: \" + str(best_l))\n",
    "    print(\"###############################################\")\n",
    "    print(\"[Last iter] training F1 was: \" + str(f1_scores_train[-1]))\n",
    "    print(\"[Last iter] dev. F1 was: \" + str(f1_scores_dev[-1]))\n",
    "    print(\"###############################################\")\n",
    "    #Dibujo lo que puedo\n",
    "    plt.plot(range(epochs), loss_per_epoch)\n",
    "    plt.plot(range(epochs), training_loss)\n",
    "    plt.plot(range(epochs), f1_scores_dev)\n",
    "    plt.plot(range(epochs), f1_scores_train)\n",
    "    plt.legend([\"Dev Loss\", \"Training Loss\", \"Dev F1\", \"Training F1\"])\n",
    "    plt.show()\n",
    "    tl = 0\n",
    "    t_pred_l = []\n",
    "    t_true_index_l = []\n",
    "    t_tokenized_l = []\n",
    "    t_text_l = []\n",
    "    for i, v in enumerate(trainloader): #Not using batches yet\n",
    "        text = torch.reshape(v['text'], (-1,))\n",
    "        tags = torch.reshape(v['spans'], (-1,))\n",
    "        optimizer.zero_grad()\n",
    "        tag_scores = model(text)\n",
    "        \n",
    "        t_pred_l.append(tag_scores.cpu().detach().numpy())\n",
    "        t_true_index_l.append([a.cpu().detach().numpy()[0] for a in v['true_index']])\n",
    "        t_tokenized_l.append([a[0] for a in v['tokenized']])\n",
    "        t_text_l.append(v['original_text'][0])\n",
    "        loss = criterion(torch.reshape(tag_scores, (-1,)), torch.reshape(tags, (-1,)).float())\n",
    "        tl += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Sentence {} done.\".format(i))\n",
    "    tl /= len(trainloader)\n",
    "    l = 0\n",
    "    print(\"Starting evaluation for loss function.\")\n",
    "    #evaluar el modelo\n",
    "    pred_l = []\n",
    "    true_index_l = []\n",
    "    tokenized_l = []\n",
    "    text_l = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for v in testloader:\n",
    "            text = torch.reshape(v['text'], (-1,))\n",
    "            tags = torch.reshape(v['spans'], (-1,))\n",
    "\n",
    "            tag_scores = model(text)\n",
    "            \n",
    "            pred_l.append(tag_scores.cpu().detach().numpy())\n",
    "            true_index_l.append([a.cpu().detach().numpy()[0] for a in v['true_index']])\n",
    "            tokenized_l.append([a[0] for a in v['tokenized']])\n",
    "            text_l.append(v['original_text'][0])\n",
    "            \n",
    "            loss = criterion(torch.reshape(tag_scores, (-1,)), torch.reshape(tags, (-1,)).float())\n",
    "            l += loss.item()\n",
    "    model.train()\n",
    "    l /= len(testloader)\n",
    "    print(\"Starting evaluation for dev F1\")\n",
    "    f1_d = f1_scores(pred_l, true_index_l, tokenized_l, text_l)\n",
    "    f1_t = f1_scores(t_pred_l, t_true_index_l, t_tokenized_l, t_text_l)\n",
    "    \n",
    "    epochs_without_change += 1\n",
    "    if best_l is None or best_l < f1_d:\n",
    "        print(\"Model improved, saving.\")\n",
    "        torch.save(model, savefile)\n",
    "        best_l = f1_d\n",
    "        best_tl = f1_t\n",
    "        epochs_without_change = 0\n",
    "        last_epoch_save = epochs\n",
    "        print(\"Model improved, saved.\")\n",
    "    #Para graficar con una escala coherente.\n",
    "    if(f1_d < worst_l_f1):\n",
    "        worst_l_f1 = f1_d\n",
    "        f1_scores_dev[0] = worst_l_f1\n",
    "    if(f1_t < worst_tl_f1):\n",
    "        worst_tl_f1 = f1_t\n",
    "        f1_scores_train[0] = worst_tl_f1\n",
    "    if(tl > worst_tl):\n",
    "        worst_tl = tl\n",
    "        training_loss[0] = worst_tl\n",
    "    if(l > worst_l):\n",
    "        worst_l = l\n",
    "        loss_per_epoch[0] = worst_l\n",
    "    #Rastreo las perdidas\n",
    "    loss_per_epoch.append(l)\n",
    "    training_loss.append(tl)\n",
    "    f1_scores_train.append(f1_t)\n",
    "    f1_scores_dev.append(f1_d)\n",
    "    #Rastreo la época actual\n",
    "    epochs += 1\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_score(train_loss, test_loss, f1_scores_train, f1_scores_test, show=False)\n",
    "plt.title('train-f1: {:.4f} \\n test-f1: {:.4f}'.format(np.max(f1_scores_train), np.max(f1_scores_test)))\n",
    "plt.savefig('Images/preprocesamiento-4.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(savefile)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv('Datos/tsd_test.csv')\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_evaluation = []\n",
    "for text in evaluation['text']:\n",
    "    tagged_sentence = tagger_LSTM(text)   \n",
    "    prediction_index = get_index_toxic_words(text.lower(), tagged_sentence)\n",
    "    indices_evaluation.append(prediction_index)\n",
    "    print(colored('Pred: ', color='cyan', attrs=['bold']) + \n",
    "           color_toxic_words(prediction_index, text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation['spans'] = indices_evaluation\n",
    "evaluation = evaluation[['spans', 'text']]\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la evaluación se debe subir un zip con un archivo txt de la siguiente manera (al final subir el archivo `spans-pred.zip` que se produce):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: spans-pred.txt (deflated 82%)\r\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluation['spans'].tolist()\n",
    "ids = evaluation.index.tolist()\n",
    "\n",
    "with open(\"spans-pred.txt\", \"w\") as out:\n",
    "    for uid, text_scores in zip(ids, predictions):\n",
    "        out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")\n",
    "        \n",
    "# Zip the predictions\n",
    "! zip -r spans-pred.zip ./spans-pred.* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
